{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vectraining import *\n",
    "import csv\n",
    "import random as rd\n",
    "vocab_size = 23940\n",
    "dim = 200\n",
    "no_ofppl = 20\n",
    "input_sentsize = 30\n",
    "input_size_bot = dim*input_sentsize + no_ofppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
    "    model = ke.Sequential()\n",
    "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
    "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "word2vec_model = word2vec()\n",
    "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "\n",
    "def shuffle_csv(path,target_path):\n",
    "    with open(path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        rows_list = []\n",
    "        for row in csvreader:\n",
    "            rows_list.append(row)\n",
    "        rd.shuffle(rows_list)\n",
    "        with open(target_path,'w', newline='') as outfile:\n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow(header)\n",
    "            csvwriter.writerows(rows_list)\n",
    "\n",
    "def fetch_rows(start,end,csv_path): #[) kinda interval\n",
    "    with open(csv_path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        tup_lst = []\n",
    "        pos = 0\n",
    "        for row in csvreader:\n",
    "            if pos >= start and pos < end:\n",
    "                \n",
    "                tup = (eval(row[1]),eval(row[2]))\n",
    "                tup_lst.append(tup)\n",
    "                \n",
    "            pos += 1\n",
    "    return tup_lst\n",
    "\n",
    "def fetch_next_batch(prev_end,batch_size,csv_path):\n",
    "    batch = fetch_rows(prev_end,prev_end+batch_size,csv_path)\n",
    "    if len(batch) != batch_size:\n",
    "        return None\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "error free 98\n",
      "Error 1\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "no_of_epochs = 1\n",
    "batch_size = 3000\n",
    "data_total_size = 307895\n",
    "\n",
    "for epoch in range(1,no_of_epochs+1):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    shuffle_csv(\"tupledata.csv\",\"shuffled_tupledata.csv\")\n",
    "    prev_end = 0\n",
    "    batch = []\n",
    "    while batch != None:\n",
    "        batch = fetch_next_batch(prev_end,batch_size,\"shuffled_tupledata.csv\")\n",
    "        if batch != None:\n",
    "            train_on_tup_batch(batch,word2vec_model)\n",
    "            prev_end = prev_end + batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

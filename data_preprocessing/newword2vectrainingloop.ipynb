{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vectraining import *\n",
    "import csv\n",
    "import random as rd\n",
    "from list_to_onehot import *\n",
    "\n",
    "vocab_size = 23940\n",
    "dim = 200\n",
    "no_ofppl = 20\n",
    "input_sentsize = 30\n",
    "input_size_bot = dim*input_sentsize + no_ofppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
    "    model = ke.Sequential()\n",
    "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
    "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "word2vec_model = word2vec()\n",
    "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "\n",
    "def shuffle_csv(path,target_path):\n",
    "    with open(path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        rows_list = []\n",
    "        for row in csvreader:\n",
    "            rows_list.append(row)\n",
    "        rd.shuffle(rows_list)\n",
    "        with open(target_path,'w', newline='') as outfile:\n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow(header)\n",
    "            csvwriter.writerows(rows_list)\n",
    "\n",
    "def fetch_rows(start,end,csv_path): #[) kinda interval\n",
    "    with open(csv_path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        tup_lst = []\n",
    "        pos = 0\n",
    "        for row in csvreader:\n",
    "            if pos >= start and pos < end:\n",
    "                \n",
    "                tup = (eval(row[1]),eval(row[2]))\n",
    "                tup_lst.append(tup)\n",
    "                \n",
    "            pos += 1\n",
    "    return tup_lst\n",
    "\n",
    "def fetch_next_batch(prev_end,batch_size,csv_path):\n",
    "    batch = fetch_rows(prev_end,prev_end+batch_size,csv_path)\n",
    "    if len(batch) != batch_size:\n",
    "        return None\n",
    "    else:\n",
    "        return batch\n",
    "\n",
    "def evaluate_model(batch_size,data_total_size,model):\n",
    "    val_batch = fetch_rows(data_total_size-batch_size,batch_size)\n",
    "    X_lst = []\n",
    "    Y_lst = []\n",
    "    for tup in val_batch:\n",
    "        x = convertInputToOneHotPercentages(tup[0])\n",
    "        y = convertInputToOneHotPercentages(tup[1])\n",
    "        X_lst.append(x)\n",
    "        Y_lst.append(y)\n",
    "    X = np.array(X_lst)\n",
    "    Y = np.array(Y_lst)\n",
    "    return model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "Epoch:  1\n",
      "([7199, 4180], [11373])\n",
      "([2252], [18430])\n",
      "([19190, 14044], [21129])\n",
      "([11143, 14683], [11200])\n",
      "([8686], [22069])\n",
      "([19789, 2969], [15365])\n",
      "([17014, 15797], [10449])\n",
      "([17109, 1896], [13439])\n",
      "([22068], [9638])\n",
      "([12650], [2764])\n",
      "([15982, 18419], [12781])\n",
      "([8921, 21114], [6520])\n",
      "([7348], [6180])\n",
      "([4023, 10741], [14457])\n",
      "([6393, 22068], [19841])\n",
      "([9694, 17465], [11138])\n",
      "([3514, 4387], [5969])\n",
      "([962], [8516])\n",
      "([16652], [4038])\n",
      "([18680, 21076], [20997])\n",
      "([23109, 19993], [21018])\n",
      "([19849], [3016])\n",
      "([7531, 6967], [17695])\n",
      "([8930, 8677], [4839])\n",
      "([20997, 19849], [7609])\n",
      "([1657, 4360], [13208])\n",
      "([913], [2186])\n",
      "([14094], [11200])\n",
      "([17098, 19738], [4795])\n",
      "([4795, 18116], [2186])\n",
      "([7504], [20696])\n",
      "([14865], [2521])\n",
      "([18602, 4355], [10514])\n",
      "([7005, 2800], [18902])\n",
      "([21753], [933])\n",
      "([2355, 21491], [8930])\n",
      "([22694, 19993], [23823])\n",
      "([7609], [12122])\n",
      "([17484], [7200])\n",
      "([2252, 2969], [5546])\n",
      "([14891, 18030], [2521])\n",
      "([16627], [17082])\n",
      "([20952, 22879], [20232])\n",
      "([21236], [6092])\n",
      "([11585], [16256])\n",
      "([14438], [23273])\n",
      "([23209, 2214], [20318])\n",
      "([15037], [7285])\n",
      "([20568], [11008])\n",
      "([19332], [14042])\n",
      "([2186, 20568], [9252])\n",
      "([6082], [2355])\n",
      "([12093], [5683])\n",
      "([2521, 4239], [5683])\n",
      "([2441, 15284], [2252])\n",
      "([21205, 7700], [21156])\n",
      "([20568, 17465], [10887])\n",
      "([4357], [4204])\n",
      "([2969, 2214], [12404])\n",
      "([729, 13185], [2428])\n",
      "([10725], [12479])\n",
      "([3646, 22069], [12335])\n",
      "([5051], [23547])\n",
      "([17112, 14594], [2969])\n",
      "([4795, 5336], [5848])\n",
      "([6088, 9975], [7580])\n",
      "([2830, 22068], [8996])\n",
      "([13862, 11713], [3133])\n",
      "([8921, 2441], [1992])\n",
      "([2742], [4360])\n",
      "([12892], [4355])\n",
      "([5443, 2441], [9041])\n",
      "([7567, 10484], [23869])\n",
      "([14621, 4158], [20060])\n",
      "([20514, 22191], [8525])\n",
      "([2800, 18419], [1950])\n",
      "([13335, 12104], [13053])\n",
      "([3661, 20232], [20788])\n",
      "([2451, 22171], [4357])\n",
      "([12870, 17720], [2355])\n",
      "([5935, 7131], [22177])\n",
      "([21063], [4387])\n",
      "([4795, 13752], [18419])\n",
      "([16661, 10121], [11373])\n",
      "([19428, 13872], [11320])\n",
      "([10466, 5683], [8420])\n",
      "([12104], [21156])\n",
      "([19650], [14865])\n",
      "([23092], [17051])\n",
      "([20183, 11200], [16572])\n",
      "([4357], [6312])\n",
      "([11210, 4357], [3809])\n",
      "([5378, 11144], [20360])\n",
      "([2302, 3131], [14107])\n",
      "([4362, 11224], [13714])\n",
      "([18796, 13875], [11200])\n",
      "([18892, 4355], [5378])\n",
      "([4795, 9566], [18419])\n",
      "([14683], [20293])\n",
      "([1423, 19934], [2244])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m batch \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     batch \u001b[39m=\u001b[39m fetch_next_batch(prev_end,batch_size,\u001b[39m\"\u001b[39m\u001b[39mshuffled_tupledata.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     train_on_tup_batch(batch,word2vec_model)\n\u001b[0;32m     15\u001b[0m     prev_end \u001b[39m=\u001b[39m prev_end \u001b[39m+\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\Parth\\humantextpatterncloner\\data_preprocessing\\word2vectraining.py:146\u001b[0m, in \u001b[0;36mtrain_on_tup_batch\u001b[1;34m(tuples_batch, model)\u001b[0m\n\u001b[0;32m    143\u001b[0m batch_model\u001b[39m.\u001b[39mtrain_on_batch(X,Y)\n\u001b[0;32m    144\u001b[0m \u001b[39m# arr2 = batch_model.layers[0].get_weights()[0].copy()\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m# print((arr1==arr2).all())\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m update_word2vec(batch_model,model)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\Parth\\humantextpatterncloner\\data_preprocessing\\word2vectraining.py:97\u001b[0m, in \u001b[0;36mupdate_word2vec\u001b[1;34m(reduced_model, model)\u001b[0m\n\u001b[0;32m     95\u001b[0m in_layer\u001b[39m.\u001b[39mset_weights([in_weights,in_bias])\n\u001b[0;32m     96\u001b[0m out_layer\u001b[39m.\u001b[39mset_weights([out_weights,out_bias])\n\u001b[1;32m---> 97\u001b[0m \u001b[39massert\u001b[39;00m (model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_weights()[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m in_layer\u001b[39m.\u001b[39mget_weights()[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mall()\n\u001b[0;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "no_of_epochs = 1\n",
    "batch_size = 100\n",
    "data_total_size = 296917\n",
    "\n",
    "for epoch in range(1,no_of_epochs+1):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    shuffle_csv(\"tupledata.csv\",\"shuffled_tupledata.csv\")\n",
    "    prev_end = 0\n",
    "    batch = []\n",
    "    while batch != None:\n",
    "        batch = fetch_next_batch(prev_end,batch_size,\"shuffled_tupledata.csv\")\n",
    "        train_on_tup_batch(batch,word2vec_model)\n",
    "        prev_end = prev_end + batch_size\n",
    "    print(\"Accuracy: \",evaluate_model(batch_size,data_total_size,word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing model\n",
    "\n",
    "val_batch = fetch_rows(data_total_size-batch_size,batch_size)\n",
    "X_lst = []\n",
    "Y_lst = []\n",
    "for tup in val_batch:\n",
    "    x = convertInputToOneHotPercentages(tup[0])\n",
    "    y = convertInputToOneHotPercentages(tup[1])\n",
    "    X_lst.append(x)\n",
    "    Y_lst.append(y)\n",
    "X = np.array(X_lst)\n",
    "Y = np.array(Y_lst)\n",
    "word2vec_model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

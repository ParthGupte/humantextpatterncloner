{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vectraining import *\n",
    "import csv\n",
    "import random as rd\n",
    "vocab_size = 23940\n",
    "dim = 200\n",
    "no_ofppl = 20\n",
    "input_sentsize = 30\n",
    "input_size_bot = dim*input_sentsize + no_ofppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
    "    model = ke.Sequential()\n",
    "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
    "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "word2vec_model = word2vec()\n",
    "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "\n",
    "def shuffle_csv(path,target_path):\n",
    "    file = open(path,'r')\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    rows_list = []\n",
    "    for row in csvreader:\n",
    "        rows_list.append(row)\n",
    "    rd.shuffle(rows_list)\n",
    "    outfile = open(target_path,'w')\n",
    "    csvwriter = csv.writer(outfile)\n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(rows_list)\n",
    "    file.close()\n",
    "\n",
    "def fetch_rows(start,end,csv_path): #[) kinda interval\n",
    "    file = open(csv_path,'r')\n",
    "    csvreader = csv.reader(file)\n",
    "    header = next(csvreader)\n",
    "    tup_lst = []\n",
    "    pos = 0\n",
    "    for row in csvreader:\n",
    "        if pos >= start and pos < end:\n",
    "            tup = (eval(row[1]),eval(row[2]))\n",
    "            tup_lst.append(tup)\n",
    "        pos += 1\n",
    "    file.close()\n",
    "    return tup_lst\n",
    "\n",
    "def fetch_next_batch(prev_end,batch_size,csv_path):\n",
    "    batch = fetch_rows(prev_end,prev_end+batch_size,csv_path)\n",
    "    if len(batch) != batch_size:\n",
    "        return None\n",
    "    else:\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "no_of_epochs = 1\n",
    "batch_size = 100\n",
    "data_total_size = 307895\n",
    "\n",
    "for epoch in range(1,no_of_epochs+1):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    shuffle_csv(\"tupledata.csv\",\"shuffled_tupledata.csv\")\n",
    "    prev_end = 0\n",
    "    while batch != None:\n",
    "        batch = fetch_next_batch(prev_end,batch_size,\"shuffled_tupledata.csv\")\n",
    "        train_on_tup_batch(batch,word2vec_model)\n",
    "        prev_end = prev_end + batch_size\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vectraining import *\n",
    "import csv\n",
    "import random as rd\n",
    "from list_to_onehot import *\n",
    "\n",
    "vocab_size = 23940\n",
    "dim = 200\n",
    "no_ofppl = 20\n",
    "input_sentsize = 30\n",
    "input_size_bot = dim*input_sentsize + no_ofppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "\n",
    "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
    "    model = ke.Sequential()\n",
    "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
    "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "word2vec_model = word2vec()\n",
    "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "\n",
    "def shuffle_csv(path,target_path):\n",
    "    with open(path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        rows_list = []\n",
    "        for row in csvreader:\n",
    "            rows_list.append(row)\n",
    "        rd.shuffle(rows_list)\n",
    "        with open(target_path,'w', newline='') as outfile:\n",
    "            csvwriter = csv.writer(outfile)\n",
    "            csvwriter.writerow(header)\n",
    "            csvwriter.writerows(rows_list)\n",
    "\n",
    "def fetch_rows(start,end,csv_path): #[) kinda interval\n",
    "    with open(csv_path,'r') as file:\n",
    "        csvreader = csv.reader(file)\n",
    "        header = next(csvreader)\n",
    "        tup_lst = []\n",
    "        pos = 0\n",
    "        for row in csvreader:\n",
    "            if pos >= start and pos < end:\n",
    "                \n",
    "                tup = (eval(row[1]),eval(row[2]))\n",
    "                tup_lst.append(tup)\n",
    "                \n",
    "            pos += 1\n",
    "    return tup_lst\n",
    "\n",
    "fetch_rows(1,10,\"shuffled_tupledata.csv\")\n",
    "\n",
    "def fetch_next_batch(prev_end,batch_size,csv_path):\n",
    "    batch = fetch_rows(prev_end,prev_end+batch_size,csv_path)\n",
    "    if len(batch) != batch_size:\n",
    "        return None\n",
    "    else:\n",
    "        return batch\n",
    "\n",
    "def evaluate_model(batch_size,data_total_size,model,csv_path):\n",
    "    val_batch = fetch_rows(data_total_size-batch_size,data_total_size,csv_path)\n",
    "    X_lst = []\n",
    "    Y_lst = []\n",
    "    dict_test = {}\n",
    "    for tup in val_batch:\n",
    "        x = convertInputToOneHotPercentages(tup[0],vocab_size)\n",
    "        y = convertInputToOneHotPercentages(tup[1],vocab_size)\n",
    "        if (type(x)==type(dict_test)):\n",
    "            item1 = list(x.values())[0]\n",
    "            X_lst.append(item1)\n",
    "        else:\n",
    "            X_lst.append(x)\n",
    "        if (type(y)==type(dict_test)):\n",
    "            item2 = list(y.values())[0]\n",
    "            Y_lst.append(item2)\n",
    "        else:\n",
    "            Y_lst.append(y)\n",
    "        \n",
    "        \n",
    "    X = np.array(X_lst)\n",
    "    print(X.shape)\n",
    "    Y = np.array(Y_lst)\n",
    "    print(Y.shape)\n",
    "    return model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 7.9447 - accuracy: 0.1240\n",
      "Accuracy:  [7.944705963134766, 0.12399999797344208]\n",
      "Epoch:  2\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 7.9294 - accuracy: 0.1140\n",
      "Accuracy:  [7.929396629333496, 0.11400000005960464]\n",
      "Epoch:  3\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 7.9025 - accuracy: 0.1250\n",
      "Accuracy:  [7.9025397300720215, 0.125]\n",
      "Epoch:  4\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 1s 15ms/step - loss: 7.8723 - accuracy: 0.1170\n",
      "Accuracy:  [7.872251987457275, 0.11699999868869781]\n",
      "Epoch:  5\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 7.7422 - accuracy: 0.1340\n",
      "Accuracy:  [7.742218494415283, 0.1340000033378601]\n",
      "Epoch:  6\n",
      "(1000, 23940)\n",
      "(1000, 23940)\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 7.7473 - accuracy: 0.1400\n",
      "Accuracy:  [7.7472944259643555, 0.14000000059604645]\n",
      "Training Complete:\n",
      "Epochs:  6\n",
      "Accuracy:  [7.7472944259643555, 0.14000000059604645]\n",
      "Time Taken:  825.5707538127899 secs\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "no_of_epochs = 6\n",
    "batch_size = 1000\n",
    "data_total_size = 296917\n",
    "tolerance = 10 #for early stopping\n",
    "\n",
    "max_acc = [0]\n",
    "unchanged_epochs = 0\n",
    "for epoch in range(1,no_of_epochs+1):\n",
    "    print(\"Epoch: \",epoch)\n",
    "    shuffle_csv(\"tupledata.csv\",\"shuffled_tupledata.csv\")\n",
    "    prev_end = 0\n",
    "    batch = []\n",
    "    while batch != None:\n",
    "        batch = fetch_next_batch(prev_end,batch_size,\"shuffled_tupledata.csv\")\n",
    "        if batch == None:\n",
    "            break\n",
    "        train_on_tup_batch(batch,word2vec_model)\n",
    "        prev_end = prev_end + batch_size\n",
    "    acc = evaluate_model(batch_size,data_total_size,word2vec_model,\"shuffled_tupledata.csv\")\n",
    "    print(\"Accuracy: \",acc)\n",
    "    if max_acc >= acc:\n",
    "        unchanged_epochs += 1\n",
    "        if unchanged_epochs >= 10:\n",
    "            break\n",
    "    else:\n",
    "        max_acc = acc\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Complete:\")\n",
    "print(\"Epochs: \",epoch)\n",
    "print(\"Accuracy: \",acc)\n",
    "print(\"Time Taken: \",end-start,\"secs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\generic_utils.py:993: RuntimeWarning: divide by zero encountered in log10\n",
      "  numdigits = int(np.log10(self.target)) + 1\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(X_lst)\n\u001b[0;32m     12\u001b[0m Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Y_lst)\n\u001b[1;32m---> 13\u001b[0m word2vec_model\u001b[39m.\u001b[39;49mevaluate(X,Y)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\generic_utils.py:993\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    990\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 993\u001b[0m     numdigits \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(np\u001b[39m.\u001b[39;49mlog10(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget)) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    994\u001b[0m     bar \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(numdigits) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39md/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m%\u001b[39m (current, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget)\n\u001b[0;32m    995\u001b[0m     prog \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(current) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget\n",
      "\u001b[1;31mOverflowError\u001b[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "#testing model\n",
    "\n",
    "val_batch = fetch_rows(data_total_size-batch_size,batch_size,\"shuffled_tupledata.csv\")\n",
    "X_lst = []\n",
    "Y_lst = []\n",
    "for tup in val_batch:\n",
    "    x = convertInputToOneHotPercentages(tup[0])\n",
    "    y = convertInputToOneHotPercentages(tup[1])\n",
    "    X_lst.append(x)\n",
    "    Y_lst.append(y)\n",
    "X = np.array(X_lst)\n",
    "Y = np.array(Y_lst)\n",
    "word2vec_model.evaluate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "dict1 = {}\n",
    "print(type(dict1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

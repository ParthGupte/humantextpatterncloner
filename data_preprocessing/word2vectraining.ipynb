{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c54a7990-12d9-4f4f-bcc9-2e277c8ac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as ke\n",
    "\n",
    "vocab_size = 23940\n",
    "dim = 200\n",
    "\n",
    "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
    "    model = ke.Sequential()\n",
    "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
    "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
    "    return model\n",
    "\n",
    "def make_reduced_model(in_out_tuple,model):\n",
    "    in_out_tuple[0].sort() #sorting for later naming reasons\n",
    "    in_out_tuple[1].sort()\n",
    "    print(in_out_tuple)\n",
    "    in_layer,out_layer = model.layers\n",
    "    in_params = in_layer.get_weights()\n",
    "    out_params = out_layer.get_weights()\n",
    "    \n",
    "    in_lst,out_lst = in_out_tuple\n",
    "    \n",
    "    in_weights = in_params[0][in_lst] #shape (2,200)\n",
    "    in_bias = in_params[1] #shape (200,)\n",
    "    out_weights = out_params[0][:,out_lst] #shape (200,1)\n",
    "    out_bias = out_params[1][out_lst] #shape (1,)\n",
    "    \n",
    "    reduced_model = ke.Sequential()\n",
    "    reduced_model._name = str(in_out_tuple)\n",
    "    #making layers of the shapes of the extracted parameters\n",
    "    h_layer = ke.layers.Dense(in_weights.shape[1], input_shape=(in_weights.shape[0],))\n",
    "    out_layer = ke.layers.Dense(out_weights.shape[1], ke.activations.softmax)\n",
    "    reduced_model.add(h_layer)\n",
    "    reduced_model.add(out_layer)\n",
    "\n",
    "    \n",
    "    #setting weights\n",
    "    reduced_model.layers[0].set_weights([in_weights,in_bias])\n",
    "    reduced_model.layers[1].set_weights([out_weights,out_bias])\n",
    "    reduced_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])\n",
    "    return reduced_model\n",
    "\n",
    "def update_word2vec(reduced_model, model):\n",
    "    in_out_tuple = eval(reduced_model.name)\n",
    "    \n",
    "    in_layer,out_layer = model.layers\n",
    "    in_params = in_layer.get_weights()\n",
    "    out_params = out_layer.get_weights()\n",
    "    \n",
    "    in_lst,out_lst = in_out_tuple\n",
    "    \n",
    "    in_weights, in_bias = in_params\n",
    "    out_weights, out_bias = out_params\n",
    "    \n",
    "    reduced_in_layer, reduced_out_layer = reduced_model.layers\n",
    "    reduced_in_params = reduced_in_layer.get_weights()\n",
    "    reduced_out_params = reduced_out_layer.get_weights()\n",
    "    \n",
    "    reduced_in_weights, reduced_in_bias = reduced_in_params\n",
    "    reduced_out_weights, reduced_out_bias = reduced_out_params\n",
    "    \n",
    "    #changing weights\n",
    "    in_weights[in_lst] = reduced_in_weights #replace rows\n",
    "    in_bias = reduced_in_bias #replace all of it\n",
    "    out_weights[:,out_lst] = reduced_out_weights #replace columns\n",
    "    out_bias[out_lst] = reduced_out_bias #replace parts of 1d array\n",
    "    \n",
    "    #update word2vec\n",
    "    in_layer.set_weights([in_weights,in_bias])\n",
    "    out_layer.set_weights([out_weights,out_bias])\n",
    "    assert (model.layers[0].get_weights()[0] == in_layer.get_weights()[0]).all()\n",
    "    \n",
    "    return model\n",
    "\n",
    "word2vec_model = word2vec()\n",
    "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b895d-7617-40e3-aa57-7b1a884488ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_tuples_batch(tuples_batch,model):\n",
    "    independent_tups = []\n",
    "    for tup in tuples_batch:\n",
    "        indpendent_tups\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "856f9f3d-4392-47a1-ad35-b265b819929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2], [1])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "red_model = make_reduced_model(([2,1],[1]),word2vec_model)\n",
    "w,b = red_model.layers[0].get_weights()\n",
    "w_c = w.copy()\n",
    "w[1,1] += 0.5\n",
    "print((w_c == w).all())\n",
    "red_model.layers[0].set_weights([w,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b4519d92-5acf-4dc2-864d-025431f8fd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f6e044efa00>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_word2vec(red_model,word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "982e2dda-1a38-4fd3-9db8-b46f09e258fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f6e044efa00>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b5cd0-65e2-443b-a55d-cee45b118968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2parth]",
   "language": "python",
   "name": "conda-env-tf2parth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

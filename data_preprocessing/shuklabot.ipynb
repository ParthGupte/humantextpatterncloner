{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tovzZ9v53b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['c:\\\\Users\\\\rohan\\\\Desktop\\\\Parth\\\\humantextpatterncloner\\\\data_preprocessing', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\python39.zip', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\DLLs', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf', '', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\Pythonwin']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "print (sys.path)\n",
        "sys.path.append('/data_preprocessing')\n",
        "from tensorflow import keras as ke\n",
        "import numpy as np\n",
        "import random as rd\n",
        "from list_to_onehot import *\n",
        "import multiprocessing as mp\n",
        "\n",
        "vocab_size = 23940\n",
        "dim = 200\n",
        "no_ofppl = 20\n",
        "input_sentsize = 30\n",
        "input_size_bot = dim*input_sentsize + no_ofppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rRNnVPLW_b5Q"
      },
      "outputs": [],
      "source": [
        "#word2vec\n",
        "\n",
        "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
        "    model = ke.Sequential()\n",
        "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
        "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UhoJOVY5P4of"
      },
      "outputs": [],
      "source": [
        "#model creation\n",
        "word2vec_model = word2vec()\n",
        "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#useful functions\n",
        "\n",
        "def func(sub_batch,L,id):\n",
        "    for n in sub_batch:\n",
        "        x_file = open(\"arraydata/x\"+n) \n",
        "        y_file = open(\"arraydata/y\"+n)\n",
        "        x_n = np.load(x_file)\n",
        "        y_n = np.load(y_file)\n",
        "        L[id].append((x_n,y_n))\n",
        "        x_file.close()\n",
        "        y_file.close()\n",
        "\n",
        "def load_batch_array(batch:list,processes = 6):\n",
        "    manager = mp.Manager()\n",
        "    L = manager.list()\n",
        "    # Ly = manager.list()\n",
        "    for i in range(processes):\n",
        "        L.append([])\n",
        "        # Ly.append([])\n",
        "\n",
        "    no_of_paths = len(batch)\n",
        "    quot = no_of_paths//processes\n",
        "    rem = no_of_paths%processes\n",
        "    sub_batch_sizes = []\n",
        "    for i in range(processes):\n",
        "        if rem != 0:\n",
        "            sub_batch_sizes.append(quot+1)\n",
        "            rem -= 1\n",
        "        else:\n",
        "            sub_batch_sizes.append(quot)\n",
        "\n",
        "    S = 0\n",
        "    proc_list = []\n",
        "    for j in range(processes):\n",
        "        sub_batch = batch[S:S+sub_batch_sizes[j]]\n",
        "        S += sub_batch_sizes[j]\n",
        "        id = j\n",
        "        p = mp.Process(target=func,args=(sub_batch,L,id))\n",
        "        proc_list.append(p)\n",
        "        p.start()\n",
        "\n",
        "    print(proc_list)\n",
        "    for p in proc_list:\n",
        "       p.join()\n",
        "    x_batch_list = []\n",
        "    y_batch_list = []\n",
        "    for l in L:\n",
        "        for (x,y) in l:\n",
        "            x_batch_list.append(x)\n",
        "            y_batch_list.append(y)\n",
        "    x_batch_arr = np.array(x_batch_list)\n",
        "    y_batch_arr = np.array(y_batch_list)\n",
        "\n",
        "    return x_batch_arr,y_batch_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "HjHizSNPQhRO",
        "outputId": "fa555c95-baa1-43dd-868d-f409f3aaf0a5"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[68], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m batch: \u001b[39m#parallelise this loop\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     x_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39marraydata\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mn\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     y_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39marraydata\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mn\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m     x_n \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(x_file,allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#training loop for when we save each array individually\n",
        "import random\n",
        "import os\n",
        "no_of_epochs = 1\n",
        "batch_size = 50\n",
        "data_total_size = 307895\n",
        "\n",
        "L = [str(x) for x in range(data_total_size)]\n",
        "L_shuffle = list(L)\n",
        "for epoch in range(1,no_of_epochs+1):\n",
        "    random.shuffle(L_shuffle)\n",
        "    for i in range(data_total_size//batch_size):\n",
        "        batch = L_shuffle[i*batch_size:(i+1)*batch_size]\n",
        "        x_batch_list = []\n",
        "        y_batch_list = []\n",
        "        for n in batch: #parallelise this loop\n",
        "            x_file = open(os.path.abspath('')+\"\\\\arraydata\\\\x\"+n+\".npy\",\"rb\")\n",
        "            y_file = open(os.path.abspath('')+\"\\\\arraydata\\\\y\"+n+\".npy\",\"rb\")\n",
        "            x_n = np.load(x_file,allow_pickle=True)\n",
        "\n",
        "\n",
        "            try:\n",
        "                x_batch_list.append(x_n.item()[0])\n",
        "            except:\n",
        "                x_batch_list.append(x_n)\n",
        "\n",
        "            y_n = np.load(y_file,allow_pickle=True)\n",
        "\n",
        "            try:\n",
        "                y_batch_list.append(y_n.item()[0])\n",
        "            except:\n",
        "                y_batch_list.append(y_n)\n",
        "\n",
        "            # x_batch_list.append(x_n)\n",
        "            # y_batch_list.append(y_n)\n",
        "            x_file.close()\n",
        "            y_file.close()\n",
        "        x_batch_array = np.array(x_batch_list)\n",
        "        y_batch_array = np.array(y_batch_list)\n",
        "        # x_batch_array, y_batch_array = load_batch_array(batch)\n",
        "        word2vec_model.train_on_batch(x_batch_array,y_batch_array)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR-ZQmlCVgB8"
      },
      "outputs": [],
      "source": [
        "word2vec_layer = word2vec_model.layers[0]\n",
        "word2vec_layer.trainable = False\n",
        "\n",
        "def tokens_to_vecs(sent:list,layer):\n",
        "    onehot_list = []\n",
        "    for word in sent:\n",
        "        onehot_list.append(convertInputToOneHotPercentages(word))\n",
        "    onehot_array = np.array(onehot_list)\n",
        "    vec_array = layer.predict(onehot_array)\n",
        "    return vec_array\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

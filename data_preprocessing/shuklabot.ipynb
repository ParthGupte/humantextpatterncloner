{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tovzZ9v53b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['c:\\\\Users\\\\rohan\\\\Desktop\\\\Parth\\\\humantextpatterncloner\\\\data_preprocessing', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\python39.zip', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\DLLs', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf', '', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\rohan\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\Pythonwin']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import sys\n",
        "print (sys.path)\n",
        "sys.path.append('/data_preprocessing')\n",
        "from tensorflow import keras as ke\n",
        "import numpy as np\n",
        "import random as rd\n",
        "from list_to_onehot import *\n",
        "import multiprocessing as mp\n",
        "\n",
        "vocab_size = 10000\n",
        "dim = 200\n",
        "no_ofppl = 20\n",
        "input_sentsize = 30\n",
        "input_size_bot = dim*input_sentsize + no_ofppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rRNnVPLW_b5Q"
      },
      "outputs": [],
      "source": [
        "#word2vec\n",
        "\n",
        "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
        "    model = ke.Sequential()\n",
        "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
        "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UhoJOVY5P4of"
      },
      "outputs": [],
      "source": [
        "#model creation\n",
        "word2vec_model = word2vec()\n",
        "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#useful functions\n",
        "\n",
        "def func(sub_batch,L,id):\n",
        "    for n in sub_batch:\n",
        "        x_file = open(\"arraydata/x\"+n) \n",
        "        y_file = open(\"arraydata/y\"+n)\n",
        "        x_n = np.load(x_file)\n",
        "        y_n = np.load(y_file)\n",
        "        L[id].append((x_n,y_n))\n",
        "        x_file.close()\n",
        "        y_file.close()\n",
        "\n",
        "def load_batch_array(batch:list,processes = 6):\n",
        "    manager = mp.Manager()\n",
        "    L = manager.list()\n",
        "    # Ly = manager.list()\n",
        "    for i in range(processes):\n",
        "        L.append([])\n",
        "        # Ly.append([])\n",
        "\n",
        "    no_of_paths = len(batch)\n",
        "    quot = no_of_paths//processes\n",
        "    rem = no_of_paths%processes\n",
        "    sub_batch_sizes = []\n",
        "    for i in range(processes):\n",
        "        if rem != 0:\n",
        "            sub_batch_sizes.append(quot+1)\n",
        "            rem -= 1\n",
        "        else:\n",
        "            sub_batch_sizes.append(quot)\n",
        "\n",
        "    S = 0\n",
        "    proc_list = []\n",
        "    for j in range(processes):\n",
        "        sub_batch = batch[S:S+sub_batch_sizes[j]]\n",
        "        S += sub_batch_sizes[j]\n",
        "        id = j\n",
        "        p = mp.Process(target=func,args=(sub_batch,L,id))\n",
        "        proc_list.append(p)\n",
        "        p.start()\n",
        "\n",
        "    for p in proc_list:\n",
        "       p.join()\n",
        "    x_batch_list = []\n",
        "    y_batch_list = []\n",
        "    for l in L:\n",
        "        for (x,y) in l:\n",
        "            x_batch_list.append(x)\n",
        "            y_batch_list.append(y)\n",
        "    x_batch_arr = np.array(x_batch_list)\n",
        "    y_batch_arr = np.array(y_batch_list)\n",
        "    return x_batch_arr,y_batch_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "HjHizSNPQhRO",
        "outputId": "fa555c95-baa1-43dd-868d-f409f3aaf0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch, 50\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Data cardinality is ambiguous:\n  x sizes: 0\n  y sizes: 6\nMake sure all arrays contain the same number of samples.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 28\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m# x_batch_list = []\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# y_batch_list = []\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# for n in batch: #parallelise this loop\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# x_batch_array = np.array(x_batch_list)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m# y_batch_array = np.array(y_batch_list)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m x_batch_array, y_batch_array \u001b[39m=\u001b[39m load_batch_array(batch)\n\u001b[1;32m---> 28\u001b[0m word2vec_model\u001b[39m.\u001b[39;49mtrain_on_batch(x_batch_array,y_batch_array)\n",
            "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2377\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   2374\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m   2375\u001b[0m     \u001b[39mself\u001b[39m\n\u001b[0;32m   2376\u001b[0m ):\n\u001b[1;32m-> 2377\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49msingle_batch_iterator(\n\u001b[0;32m   2378\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2379\u001b[0m     )\n\u001b[0;32m   2380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m   2381\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n",
            "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1831\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1828\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1829\u001b[0m     data \u001b[39m=\u001b[39m (x, y, sample_weight)\n\u001b[1;32m-> 1831\u001b[0m _check_data_cardinality(data)\n\u001b[0;32m   1832\u001b[0m dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensors(data)\n\u001b[0;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m class_weight:\n",
            "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1851\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1845\u001b[0m         label,\n\u001b[0;32m   1846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1847\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1848\u001b[0m         ),\n\u001b[0;32m   1849\u001b[0m     )\n\u001b[0;32m   1850\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1851\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
            "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 0\n  y sizes: 6\nMake sure all arrays contain the same number of samples."
          ]
        }
      ],
      "source": [
        "#training loop for when we save each array individually\n",
        "import random\n",
        "no_of_epochs = 1\n",
        "batch_size = 50\n",
        "data_total_size = 307895\n",
        "\n",
        "L = [str(x) for x in range(data_total_size)]\n",
        "L_shuffle = list(L)\n",
        "for epoch in range(1,no_of_epochs+1):\n",
        "    random.shuffle(L_shuffle)\n",
        "    for i in range(data_total_size//batch_size):\n",
        "        batch = L_shuffle[i*batch_size:(i+1)*batch_size]\n",
        "        print(\"batch,\",batch)\n",
        "        # x_batch_list = []\n",
        "        # y_batch_list = []\n",
        "        # for n in batch: #parallelise this loop\n",
        "        #     x_file = open(\"arraydata/x\"+n) \n",
        "        #     y_file = open(\"arraydata/y\"+n)\n",
        "        #     x_n = np.load(x_file)\n",
        "        #     y_n = np.load(y_file)\n",
        "        #     x_batch_list.append(x_n)\n",
        "        #     y_batch_list.append(y_n)\n",
        "        #     x_file.close()\n",
        "        #     y_file.close()\n",
        "        # x_batch_array = np.array(x_batch_list)\n",
        "        # y_batch_array = np.array(y_batch_list)\n",
        "        x_batch_array, y_batch_array = load_batch_array(batch)\n",
        "        word2vec_model.train_on_batch(x_batch_array,y_batch_array)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR-ZQmlCVgB8"
      },
      "outputs": [],
      "source": [
        "word2vec_layer = word2vec_model.layers[0]\n",
        "word2vec_layer.trainable = False\n",
        "\n",
        "def tokens_to_vecs(sent:list,layer):\n",
        "    onehot_list = []\n",
        "    for word in sent:\n",
        "        onehot_list.append(convertInputToOneHotPercentages(word))\n",
        "    onehot_array = np.array(onehot_list)\n",
        "    vec_array = layer.predict(onehot_array)\n",
        "    return vec_array\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tovzZ9v53b66"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras as ke\n",
        "import numpy as np\n",
        "import random as rd\n",
        "from list_to_onehot import *\n",
        "import multiprocessing as mp\n",
        "\n",
        "vocab_size = 10000\n",
        "dim = 200\n",
        "no_ofppl = 20\n",
        "input_sentsize = 30\n",
        "input_size_bot = dim*input_sentsize + no_ofppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRNnVPLW_b5Q"
      },
      "outputs": [],
      "source": [
        "#word2vec\n",
        "\n",
        "def word2vec(vocab_size = vocab_size,h_size = dim):\n",
        "    model = ke.Sequential()\n",
        "    model.add(ke.layers.Dense(h_size, input_shape = (vocab_size,)))\n",
        "    model.add(ke.layers.Dense(vocab_size,activation = ke.activations.softmax))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhoJOVY5P4of"
      },
      "outputs": [],
      "source": [
        "#model creation\n",
        "word2vec_model = word2vec()\n",
        "word2vec_model.compile(loss=\"categorical_crossentropy\",optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#useful functions\n",
        "\n",
        "def func(sub_batch,Lx,Ly,id):\n",
        "    for n in sub_batch:\n",
        "        x_file = open(\"arraydata/x\"+n) \n",
        "        y_file = open(\"arraydata/y\"+n)\n",
        "        x_n = np.load(x_file)\n",
        "        y_n = np.load(y_file)\n",
        "        Lx[id].append(x_n)\n",
        "        Ly[id].append(y_n)\n",
        "        x_file.close()\n",
        "        y_file.close()\n",
        "\n",
        "def load_batch_array(batch:list,processes = 6):\n",
        "    manager = mp.Manager()\n",
        "    Lx = manager.list()\n",
        "    Ly = manager.list()\n",
        "    for i in range(processes):\n",
        "        Lx.append([])\n",
        "        Ly.append([])\n",
        "\n",
        "    no_of_paths = len(batch)\n",
        "    quot = no_of_paths//processes\n",
        "    rem = no_of_paths%processes\n",
        "    sub_batch_sizes = []\n",
        "    for i in range(processes):\n",
        "        if rem != 0:\n",
        "            sub_batch_sizes.append(quot+1)\n",
        "            rem -= 1\n",
        "        else:\n",
        "            sub_batch_sizes.append(quot)\n",
        "\n",
        "    S = 0\n",
        "    proc_list = []\n",
        "    for j in range(processes):\n",
        "        sub_batch = batch[S:S+sub_batch_sizes[j]]\n",
        "        S += sub_batch_sizes[j]\n",
        "        id = j\n",
        "        p = mp.Process(target=func,args=(sub_batch,Lx,Ly,id))\n",
        "        proc_list.append(p)\n",
        "        p.start()\n",
        "\n",
        "    for p in proc_list:\n",
        "       p.join()\n",
        "    x_batch_list = []\n",
        "    y_batch_list = []\n",
        "    for lx in Lx:\n",
        "        x_batch_list.extend(lx)\n",
        "    for ly in Ly:\n",
        "        y_batch_list.append(ly)\n",
        "    x_batch_arr = np.array(x_batch_list)\n",
        "    y_batch_arr = np.array(y_batch_list)\n",
        "    return x_batch_arr,y_batch_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "HjHizSNPQhRO",
        "outputId": "fa555c95-baa1-43dd-868d-f409f3aaf0a5"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1c57caff7671>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mchoicelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoicelist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mchoicelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
          ]
        }
      ],
      "source": [
        "#training loop for when we save each array individually\n",
        "\n",
        "no_of_epochs = 100\n",
        "batch_size = 50\n",
        "data_total_size = 10000\n",
        "\n",
        "L = [str(x) for x in range(data_total_size)]\n",
        "for epoch in range(1,no_of_epochs+1):\n",
        "    L_shuffle = rd.shuffle(L)\n",
        "    for i in range(data_total_size//batch_size):\n",
        "        batch = L_shuffle[i*batch_size:(i+1)*batch_size]\n",
        "        # x_batch_list = []\n",
        "        # y_batch_list = []\n",
        "        # for n in batch: #parallelise this loop\n",
        "        #     x_file = open(\"arraydata/x\"+n) \n",
        "        #     y_file = open(\"arraydata/y\"+n)\n",
        "        #     x_n = np.load(x_file)\n",
        "        #     y_n = np.load(y_file)\n",
        "        #     x_batch_list.append(x_n)\n",
        "        #     y_batch_list.append(y_n)\n",
        "        #     x_file.close()\n",
        "        #     y_file.close()\n",
        "        # x_batch_array = np.array(x_batch_list)\n",
        "        # y_batch_array = np.array(y_batch_list)\n",
        "        x_batch_array, y_batch_array = load_batch_array(batch)\n",
        "        word2vec_model.train_on_batch(x_batch_array,y_batch_array)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR-ZQmlCVgB8"
      },
      "outputs": [],
      "source": [
        "word2vec_layer = word2vec_model.layers[0]\n",
        "word2vec_layer.trainable = False\n",
        "\n",
        "def tokens_to_vecs(sent:list,layer):\n",
        "    onehot_list = []\n",
        "    for word in sent:\n",
        "        onehot_list.append(convertInputToOneHotPercentages(word))\n",
        "    onehot_array = np.array(onehot_list)\n",
        "    vec_array = layer.predict(onehot_array)\n",
        "    return vec_array\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
